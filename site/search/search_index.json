{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"This is home page","text":"<p>Acknowledgment: @HobbitQia</p>"},{"location":"RL/PG/","title":"Policy Gradient","text":""},{"location":"RL/PG/#policy","title":"Policy","text":"<p>Policy \ud835\udf0b is a network with parameter \ud835\udf03 </p> <ul> <li>Input: the observation of machine represented as a vector or a matrix </li> <li>Output: each action corresponds to a neuron in output layer</li> </ul>"},{"location":"RL/PG/#trajectory","title":"Trajectory","text":"<ul> <li> <p>Trajectory $$\\tau = {{s_1, a_1, s_2, a_2, \\cdots, s_T, a_T}} $$</p> </li> <li> \\[ p_{\\theta}(\\tau)= p(s_1)p_{\\theta}(a_1|s_1)p(s_2|s_1, a_1)p_{\\theta}(a_2|s_2)p(s_3|s_2, a_2)\\cdots = p(s_1)\\prod_{t=1}^{T} p_{\\theta}(a_t|s_t)p(s_{t+1}|s_t, a_t) \\] </li> </ul>"}]}