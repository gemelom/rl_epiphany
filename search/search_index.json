{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"This is home page","text":"<p>Acknowledgment: @HobbitQia</p>"},{"location":"RL/PG/","title":"PG","text":""},{"location":"RL/PG/#policy-gradient","title":"Policy Gradient","text":""},{"location":"RL/PG/#policy","title":"Policy","text":"<p>Policy \ud835\udf0b is a network with parameter \ud835\udf03 </p> <ul> <li>Input: the observation of machine represented as a vector or a matrix</li> <li>Output: each action corresponds to a neuron in output layer</li> </ul>"},{"location":"RL/PG/#trajectory","title":"Trajectory","text":"<ul> <li> <p>Trajectory \u200b</p> </li> <li> <p>Probability of  </p> </li> <li> <p>Expected reward     </p> </li> </ul> Note <p>Target : find an actor to maximize the expectation of accumulating reward  </p>"},{"location":"RL/PG/#policy-gradient_1","title":"Policy gradient","text":"<ul> <li> <p>Calculate grad    </p> </li> <li> <p>Update model    </p> </li> </ul>"},{"location":"RL/PG/#implementation","title":"Implementation","text":"<ul> <li>Tip1: Add a baseline    </li> <li>Tip2: Assign suitable credit    </li> </ul>"}]}