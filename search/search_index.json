{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"This is home page","text":"<p>\ud83c\udf89\ud83c\udf89\ud83c\udf89 <pre><code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557     \u2588\u2588\u2557\u2588\u2588\u2557   \u2588\u2588\u2557             \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n\u255a\u2550\u2550\u2588\u2588\u2588\u2554\u255d     \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551            \u2588\u2588\u2588\u2551 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\n  \u2588\u2588\u2588\u2554\u255d      \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551            \u255a\u2588\u2588\u2551  \u2588\u2588\u2588\u2588\u2588\u2554\u255d    \u2588\u2588\u2554\u255d\n \u2588\u2588\u2588\u2554\u255d  \u2588\u2588   \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551             \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2550\u255d    \u2588\u2588\u2554\u255d \n \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u255a\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d             \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551  \n \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d              \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d  \n</code></pre></p> <p>Acknowledgment: @HobbitQia</p>"},{"location":"RL/PG/","title":"PG","text":""},{"location":"RL/PG/#policy-gradient","title":"Policy Gradient","text":""},{"location":"RL/PG/#policy","title":"Policy","text":"<p>Policy \ud835\udf0b is a network with parameter \ud835\udf03 </p> <ul> <li>Input: the observation of machine represented as a vector or a matrix</li> <li>Output: each action corresponds to a neuron in output layer</li> </ul>"},{"location":"RL/PG/#trajectory","title":"Trajectory","text":"<ul> <li> <p>Trajectory \u200b</p> </li> <li> <p>Probability of  </p> </li> <li> <p>Expected reward     </p> </li> </ul> Note <p>Target : find an actor to maximize the expectation of accumulating reward  </p>"},{"location":"RL/PG/#policy-gradient_1","title":"Policy gradient","text":"<ul> <li> <p>Calculate grad    </p> </li> <li> <p>Update model    </p> </li> </ul>"},{"location":"RL/PG/#implementation","title":"Implementation","text":"<ul> <li>Tip1: Add a baseline    </li> <li>Tip2: Assign suitable credit    </li> </ul>"},{"location":"RL/PPO/","title":"PPO","text":""},{"location":"RL/PPO/#ppo","title":"PPO","text":""},{"location":"RL/PPO/#on-policy-to-off-policy","title":"On-policy to Off-policy","text":"<ul> <li>On-policy: The agent learned and the agent interacting with the environment is the same.</li> <li>Off-policy: The agent learned and the agent interacting with the environment is different.</li> </ul>"},{"location":"RL/PPO/#importance-sampling","title":"Importance sampling","text":"<p>We can have  sampled from  instead of , by multiplying by an importance weight  </p>"},{"location":"RL/PPO/#on-policy-off-policy","title":"On-policy -&gt; Off-policy","text":"<ul> <li>Gradiant  </li> <li>Target  </li> </ul>"},{"location":"RL/PPO/#ppo_1","title":"PPO","text":"<p>  cannot be very different from , so need to add constraint on behavior  </p>"}]}