{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"This is home page","text":"<p>Acknowledgment: @HobbitQia</p>"},{"location":"RL/DQN/","title":"DQN","text":""},{"location":"RL/DQN/#dqn","title":"DQN","text":""},{"location":"RL/DQN/#replay-buffer","title":"Replay Buffer","text":"<p>Ensure i.i.d. training data</p> <ul> <li>Store  into buffer every timestep</li> <li>Sample  batch from buffer every train frequency step</li> </ul>"},{"location":"RL/DQN/#target-network","title":"Target Network","text":"<p>Fixed network  to calculate TD target, every  steps  </p> <p> </p>"},{"location":"RL/DQN/#double-dqn","title":"Double DQN","text":"<p>With random noise  </p> <ul> <li> <p>Q value is always over-estimated   </p> </li> <li> <p>Double DQN: two functions  and  </p> <ul> <li>if  over-estimate a,  would possibly give it proper value</li> <li>if  over-estimate a,  would possibly not select it</li> </ul> </li> </ul>"},{"location":"RL/DQN/#dueling-dqn","title":"Dueling DQN","text":"<p>Split the original  network into two parts:  network and  network</p> <p> </p> <ul> <li> : represent state value</li> <li> : measure the rationality of selecting a specific action in a certain state</li> </ul>"},{"location":"RL/DQN/#prioritized-replay","title":"Prioritized Replay","text":"<p>The data with larger TD error in previous training has higher probability to be sampled</p>"},{"location":"RL/PG/","title":"PG","text":""},{"location":"RL/PG/#policy-gradient","title":"Policy Gradient","text":""},{"location":"RL/PG/#policy","title":"Policy","text":"<p>Policy \ud835\udf0b is a network with parameter \ud835\udf03 </p> <ul> <li>Input: the observation of machine represented as a vector or a matrix</li> <li>Output: each action corresponds to a neuron in output layer</li> </ul>"},{"location":"RL/PG/#trajectory","title":"Trajectory","text":"<ul> <li> <p>Trajectory \u200b</p> </li> <li> <p>Probability of  </p> </li> <li> <p>Expected reward     </p> </li> </ul> Note <p>Target : find an actor to maximize the expectation of accumulating reward  </p>"},{"location":"RL/PG/#policy-gradient_1","title":"Policy gradient","text":"<ul> <li> <p>Calculate grad    </p> </li> <li> <p>Update model    </p> </li> </ul>"},{"location":"RL/PG/#implementation","title":"Implementation","text":"<ul> <li>Tip1: Add a baseline    </li> <li>Tip2: Assign suitable credit    </li> </ul>"},{"location":"RL/PPO/","title":"PPO","text":""},{"location":"RL/PPO/#ppo","title":"PPO","text":""},{"location":"RL/PPO/#on-policy-to-off-policy","title":"On-policy to Off-policy","text":"<ul> <li>On-policy: The agent learned and the agent interacting with the environment is the same.</li> <li>Off-policy: The agent learned and the agent interacting with the environment is different.</li> </ul>"},{"location":"RL/PPO/#importance-sampling","title":"Importance sampling","text":"<p>We can have  sampled from  instead of , by multiplying by an importance weight  </p> Note <p>Issue of importance sampling  Thus  should not be too far away from  </p>"},{"location":"RL/PPO/#on-policy-off-policy","title":"On-policy -&gt; Off-policy","text":"<ul> <li> <p>Gradiant  </p> </li> <li> <p>Target  </p> </li> </ul>"},{"location":"RL/PPO/#ppo_1","title":"PPO","text":"<p>  cannot be very different from , so need to add constraint on behavior  </p>"},{"location":"RL/Q-Learning/","title":"Q Learning","text":""},{"location":"RL/Q-Learning/#q-learning","title":"Q-Learning","text":""},{"location":"RL/Q-Learning/#mc-vs-td","title":"MC v.s. TD","text":"<ul> <li> <p>MC  Large variance,  is the summation of many steps</p> </li> <li> <p>TD  Smaller variance, maybe inaccurate</p> </li> </ul>"},{"location":"RL/Q-Learning/#q-learning_1","title":"Q-Learning","text":"<p>State-action value function : when using actor  , the cumulated reward expects to be obtained after taking  at state  </p> <p> </p> Note <p>Proof  </p> <p>Compared with SARSA:</p> <p> </p> <ul> <li>Both use TD control</li> <li>SARSA is on-policy(TD target from same policy), while Q-learning is off-policy(TD target generated by total greedy method)</li> </ul>"}]}