{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"This is home page","text":"<p>Acknowledgment: @HobbitQia</p>"},{"location":"PTCG/env/","title":"Environment","text":""},{"location":"PTCG/env/#observation-space","title":"Observation space","text":"<ul> <li>dim=(283,), dtype=uint8</li> <li>four main components:<ul> <li>agent state feature (0~130)</li> <li>opponent state feature (131~259)</li> <li>agent private feature (260~279)</li> <li>common feature (280~282)</li> </ul> </li> </ul> begin size name high_level desc range 0 1 active_pokemon_num agent state feature [0, 1] 1 1 bench_pokemon_num [0, 6] 2 1 left_prize_num [0, 6] 3 1 hand_num [0, 60] 4 1 deck_num [0, 60] 5 1 discard_num [0, 60] 6 1 lost_zone_num [0, 60] 7 1 vstar_used 0 - used 1 - not used [0, 1] 8 1 supporter_used 0 - disable 1 - available [0, 1] 9 1 energy_attached 0 - disable 1 - available [0, 1] 10 1 active_pokemon_exist [0, 1] 11 1 active_pokemon_hp [0, 400] 12 1 active_pokemon_damage_taken [0, 400] 13 1 active_pokemon_card_type [0, 10] 14 1 active_pokemon_weakness [0, 10] 15 1 active_pokemon_resistence [0, 10] 16 1 active_pokemon_prize [0, 3] 17 1 active_pokemon_has_ability [0, 1] 18 1 active_pokemon_ability_used [0, 1] 19 1 active_pokemon_stage 0 - basic 1 - stage1 2 - stage2 [0, 2] 20 1 active_pokemon_evolve_available [0, 1] 21 1 active_pokemon_has_item [0, 1] 22 1 active_pokemon_condition [0, 4] 23 1 active_pokemon_retreat_available [0, 1] 24 1 active_pokemon_skill1_available [0, 1] 25 1 active_pokemon_skill2_available [0, 1] 26 1 active_pokemon_skill3_available [0, 1] 27 1 active_pokemon_item_skill_available [0, 1] 28 1 bench_pokemon_exist [0, 1] 29 1 bench_pokemon_hp [0, 400] 30 1 bench_pokemon_damage_taken [0, 400] 31 1 bench_pokemon_card_type [0, 10] 32 1 bench_pokemon_weakness [0, 10] 33 1 bench_pokemon_resistence [0, 10] 34 1 bench_pokemon_prize [0, 3] 35 1 bench_pokemon_has_ability [0, 1] 36 1 bench_pokemon_ability_used [0, 1] 37 1 bench_pokemon_stage 0 - basic 1 - stage1 2 - stage2 [0, 2] 38 1 bench_pokemon_evolve_available [0, 1] 39 1 bench_pokemon_has_item [0, 1] 40 1 bench_pokemon_condition [0, 4] 41 1 bench_pokemon_retreat_available [0, 1] 42 1 bench_pokemon_skill1_available [0, 1] 43 1 bench_pokemon_skill2_available [0, 1] 44 1 bench_pokemon_skill3_available [0, 1] 45 1 bench_pokemon_item_skill_available [0, 1] 46 85 ... other 5 bench 131 1 active_pokemon_num opponent state feature [0, 1] 132 1 bench_pokemon_num [0, 6] 133 1 left_prize_num [0, 6] 134 1 hand_num [0, 60] 135 1 deck_num [0, 60] 136 1 discard_num [0, 60] 137 1 lost_zone_num [0, 60] 138 1 vstar_used 0 - used 1 - not used [0, 1] 139 1 active_pokemon_exist [0, 1] 140 1 active_pokemon_hp [0, 400] 141 1 active_pokemon_damage_taken [0, 400] 142 1 active_pokemon_card_type [0, 10] 143 1 active_pokemon_weakness [0, 10] 144 1 active_pokemon_resistence [0, 10] 145 1 active_pokemon_prize [0, 3] 146 1 active_pokemon_has_ability [0, 1] 147 1 active_pokemon_ability_used [0, 1] 148 1 active_pokemon_stage 0 - basic 1 - stage1 2 - stage2 [0, 2] 149 1 active_pokemon_evolve_available [0, 1] 150 1 active_pokemon_has_item [0, 1] 151 1 active_pokemon_condition [0, 4] 152 1 active_pokemon_retreat_available [0, 1] 153 1 active_pokemon_skill1_available [0, 1] 154 1 active_pokemon_skill2_available [0, 1] 155 1 active_pokemon_skill3_available [0, 1] 156 1 active_pokemon_item_skill_available [0, 1] 157 1 bench_pokemon_exist [0, 1] 158 1 bench_pokemon_hp [0, 400] 159 1 bench_pokemon_damage_taken [0, 400] 160 1 bench_pokemon_card_type [0, 10] 161 1 bench_pokemon_weakness [0, 10] 162 1 bench_pokemon_resistence [0, 10] 163 1 bench_pokemon_prize [0, 3] 164 1 bench_pokemon_has_ability [0, 1] 165 1 bench_pokemon_ability_used [0, 1] 166 1 bench_pokemon_stage 0 - basic 1 - stage1 2 - stage2 [0, 2] 167 1 bench_pokemon_evolve_available [0, 1] 168 1 bench_pokemon_has_item [0, 1] 169 1 bench_pokemon_condition [0, 4] 170 1 bench_pokemon_retreat_available [0, 1] 171 1 bench_pokemon_skill1_available [0, 1] 172 1 bench_pokemon_skill2_available [0, 1] 173 1 bench_pokemon_skill3_available [0, 1] 174 1 bench_pokemon_item_skill_available [0, 1] 175 85 ... other 5 bench 260 1 has_energy agent private feature [0, 1] 261 1 has_supporter [0, 1] 262 1 has_pokemon_tool [0, 1] 263 1 has_basic_pokemon [0, 1] 264 1 has_evolve_pokemon [0, 1] 265 1 has_stadium [0, 1] 266 1 has_item [0, 1] 267 1 has_switch_agent [0, 1] 268 1 has_switch_opponent [0, 1] 269 1 has_basic_ball [0, 1] 270 1 has_special_ball [0, 1] 271 1 has_shuffle [0, 1] 272 1 has_find_trainer [0, 1] 273 1 has_recycle_energy [0, 1] 274 1 has_recycle_pokemon [0, 1] 275 1 has_recycle_trainer [0, 1] 276 1 energy_num [0, 60] 277 1 pokemon_num [0, 60] 278 1 trainer_num [0, 60] 279 1 supporter_num [0, 60] 280 1 has_stadium common feature [0, 1] 281 1 from_whom_stadium 0 - from opponent 1 - from agent [0, 1] 282 1 who_play_first 0 - opponent 1 - agent [0, 1]"},{"location":"PTCG/env/#action-space","title":"Action space","text":""},{"location":"RL/DQN/","title":"DQN","text":""},{"location":"RL/DQN/#replay-buffer","title":"Replay Buffer","text":"<p>Ensure i.i.d. training data</p> <ul> <li>Store  into buffer every timestep</li> <li>Sample  batch from buffer every train frequency step</li> </ul>"},{"location":"RL/DQN/#target-network","title":"Target Network","text":"<p>Fixed network  to calculate TD target, every  steps  </p> <p> </p>"},{"location":"RL/DQN/#double-dqn","title":"Double DQN","text":"<p>With random noise  </p> <ul> <li> <p>Q value is always over-estimated   </p> </li> <li> <p>Double DQN: two functions  and  </p> <ul> <li>if  over-estimate a,  would possibly give it proper value</li> <li>if  over-estimate a,  would possibly not select it</li> </ul> </li> </ul>"},{"location":"RL/DQN/#dueling-dqn","title":"Dueling DQN","text":"<p>Split the original  network into two parts:  network and  network</p> <p> </p> <ul> <li> : represent state value</li> <li> : measure the rationality of selecting a specific action in a certain state</li> </ul>"},{"location":"RL/DQN/#prioritized-replay","title":"Prioritized Replay","text":"<p>The data with larger TD error in previous training has higher probability to be sampled</p>"},{"location":"RL/PG/","title":"Policy Gradient","text":""},{"location":"RL/PG/#policy","title":"Policy","text":"<p>Policy \ud835\udf0b is a network with parameter \ud835\udf03 </p> <ul> <li>Input: the observation of machine represented as a vector or a matrix</li> <li>Output: each action corresponds to a neuron in output layer</li> </ul>"},{"location":"RL/PG/#trajectory","title":"Trajectory","text":"<ul> <li> <p>Trajectory \u200b</p> </li> <li> <p>Probability of  </p> </li> <li> <p>Expected reward     </p> </li> </ul> Note <p>Target : find an actor to maximize the expectation of accumulating reward  </p>"},{"location":"RL/PG/#policy-gradient_1","title":"Policy gradient","text":"<ul> <li> <p>Calculate grad    </p> </li> <li> <p>Update model    </p> </li> </ul>"},{"location":"RL/PG/#implementation","title":"Implementation","text":"<ul> <li>Tip1: Add a baseline    </li> <li>Tip2: Assign suitable credit    </li> </ul>"},{"location":"RL/PPO/","title":"PPO","text":""},{"location":"RL/PPO/#on-policy-to-off-policy","title":"On-policy to Off-policy","text":"<ul> <li>On-policy: The agent learned and the agent interacting with the environment is the same.</li> <li>Off-policy: The agent learned and the agent interacting with the environment is different.</li> </ul>"},{"location":"RL/PPO/#importance-sampling","title":"Importance sampling","text":"<p>We can have  sampled from  instead of , by multiplying by an importance weight  </p> Note <p>Issue of importance sampling  Thus  should not be too far away from  </p>"},{"location":"RL/PPO/#on-policy-off-policy","title":"On-policy -&gt; Off-policy","text":"<ul> <li> <p>Gradiant  </p> </li> <li> <p>Target  </p> </li> </ul>"},{"location":"RL/PPO/#ppo_1","title":"PPO","text":"<p>  cannot be very different from , so need to add constraint on behavior  </p>"},{"location":"RL/Q-Learning/","title":"Q-Learning","text":""},{"location":"RL/Q-Learning/#mc-vs-td","title":"MC v.s. TD","text":"<ul> <li> <p>MC  Large variance,  is the summation of many steps</p> </li> <li> <p>TD  Smaller variance, maybe inaccurate</p> </li> </ul>"},{"location":"RL/Q-Learning/#q-learning_1","title":"Q-Learning","text":"<p>State-action value function : when using actor  , the cumulated reward expects to be obtained after taking  at state  </p> <p> </p> Note <p>Proof  </p> <p>Compared with SARSA:</p> <p> </p> <ul> <li>Both use TD control</li> <li>SARSA is on-policy(TD target from same policy), while Q-learning is off-policy(TD target generated by total greedy method)</li> </ul>"}]}